{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e9a465",
   "metadata": {},
   "source": [
    "# Bag of words pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafff315",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5ad075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f18b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3963d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Pritish is a peaceful soul\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2673b29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pritish', 'is', 'a', 'peaceful', 'soul']\n"
     ]
    }
   ],
   "source": [
    "wo = word_tokenize(sentence)\n",
    "print(wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe698fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"It was a very pleasant day. The weather was cool and windy. I went to market to buy grocery.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa47d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day.', 'The weather was cool and windy.', 'I went to market to buy grocery.']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(document)\n",
    "print(sents)\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb38a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a very pleasant day.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e74a891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pritish', 'is', 'a', 'peaceful', 'soul']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bd166",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d31e007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74614380",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "428ebe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"she's\", 'to', 'doing', 'only', 'her', 'any', 'here', 'not', 'yours', \"doesn't\", 'mightn', 'its', 'can', 'after', 'where', \"it's\", 'it', 'they', 'herself', \"aren't\", \"won't\", 'ourselves', 'theirs', \"you'll\", 'whom', 'were', \"mightn't\", 'this', 'hadn', 'why', 'ain', 'couldn', 'but', 'having', 'at', 'off', \"that'll\", 'how', 'own', 'she', 'as', 'needn', \"you'd\", \"couldn't\", 'during', \"wouldn't\", 'over', 'once', 'from', \"hadn't\", \"shan't\", 'up', 'shouldn', 'that', 'my', 'in', 'am', 'such', 'will', \"don't\", \"weren't\", 'than', 'who', 'and', 'below', 'ma', 're', 'his', 'against', 'are', 'be', 'an', 'same', 'mustn', 'wouldn', 'under', \"hasn't\", 'on', 'just', 'yourself', 'our', 'themselves', 'all', 'too', 'o', 'there', 'hasn', 'should', 'is', 'again', \"should've\", 'isn', 'hers', 'if', 'nor', 'didn', 'he', 'has', 'down', 'those', 'i', 'no', 'itself', 'because', 'being', 'we', 'a', 'each', 'doesn', 'between', 'through', 'll', 'yourselves', 'their', 'for', 'both', 'haven', 'the', 'or', 'what', 'other', 'these', \"isn't\", \"wasn't\", \"mustn't\", 'further', 'very', 'when', 'out', 'into', 'some', 'most', 'wasn', 'd', 'by', 'with', 'which', 'aren', 'weren', 'above', 'you', 'does', 'ours', \"haven't\", 'more', \"you're\", 'now', 'been', \"shouldn't\", 'them', 'himself', 's', 'don', 'shan', 'myself', 'about', 'won', 'so', 'of', 'then', 'before', 'y', 'your', 'while', 'few', 'had', 'm', 'him', 'me', \"you've\", 've', 'until', 'was', \"didn't\", 'did', 't', 'have', 'do', \"needn't\"}\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9219442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sw contains the common words we can skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d8e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(text, stopwords):\n",
    "    useful_words = [w for w in text if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a9b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bothered', 'much']\n"
     ]
    }
   ],
   "source": [
    "text = \"i am not bothered about her so much\".split()\n",
    "useful_text = removeStopwords(text, sw)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab817d1",
   "metadata": {},
   "source": [
    "## Tokenization using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e691b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Send all the 50 documents of chapter 1,2,3 to pritishpattnaik7@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d2f94ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ee4dd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'of', 'chapter', 'to', 'pritishpattnaik', '@gmail', 'com']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z@]+')\n",
    "useful_texts = tokenizer.tokenize(sentence1)\n",
    "print(useful_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42bdaa",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8917fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"Foxes loves to make jumps. A quick brown fox was seen jumping over the lovely dog from a 6ft high wall.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f681868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#types of stemming - Snowball , porter , lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e640c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea4241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()   #object creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83f8cec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9798bbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac91aecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc58f4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('loving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ad59954",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b69dfec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "416b5469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('playful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65501315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0eac40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccb57e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'playful'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn = WordNetLemmatizer()\n",
    "wn.lemmatize('playful')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b2abb",
   "metadata": {},
   "source": [
    "## Building vocab and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3dab5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Indian cricket team will win the world cup says capt. Virat Kohili. World cup will be held at Srilanka.',\n",
    "    'We will win the next lok sabha eelections, says confidient pm Narendra Modi.',\n",
    "    'The nobel laurate won the hearts of people.',\n",
    "    'The movie Raazi is an excelent spy movie.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c6125f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9ad643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3f52408",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ebaa786",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f6f3dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 2, 1, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc63004c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 11,\n",
       " 'cricket': 5,\n",
       " 'team': 29,\n",
       " 'will': 33,\n",
       " 'win': 34,\n",
       " 'the': 30,\n",
       " 'world': 36,\n",
       " 'cup': 6,\n",
       " 'says': 26,\n",
       " 'capt': 3,\n",
       " 'virat': 31,\n",
       " 'kohili': 13,\n",
       " 'be': 2,\n",
       " 'held': 10,\n",
       " 'at': 1,\n",
       " 'srilanka': 28,\n",
       " 'we': 32,\n",
       " 'next': 19,\n",
       " 'lok': 15,\n",
       " 'sabha': 25,\n",
       " 'eelections': 7,\n",
       " 'confidient': 4,\n",
       " 'pm': 23,\n",
       " 'narendra': 18,\n",
       " 'modi': 16,\n",
       " 'nobel': 20,\n",
       " 'laurate': 14,\n",
       " 'won': 35,\n",
       " 'hearts': 9,\n",
       " 'of': 21,\n",
       " 'people': 22,\n",
       " 'movie': 17,\n",
       " 'raazi': 24,\n",
       " 'is': 12,\n",
       " 'an': 0,\n",
       " 'excelent': 8,\n",
       " 'spy': 27}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7ac7a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(cv.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aad15b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de89d02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = vectorized_corpus[2]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08ba98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d9aa8f1",
   "metadata": {},
   "source": [
    "## Vectorization with Stopword removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c57dd3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(doc):\n",
    "    words = tokenizer.tokenize(doc.lower())\n",
    "    #Indian and indian = same\n",
    "    \n",
    "    #remove stopwords\n",
    "    words = removeStopwords(words, sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7cad72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pritish', 'peaceful', 'soul']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f32b79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "46ba215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus1 = cv1.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66a879ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 2 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 2]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 1 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8f96ea3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d7e6e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here the length of vector is reduced (we have made it more efficient we do it with stopword removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e874bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89fb28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
